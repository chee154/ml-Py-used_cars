{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini-Project 1 - Used Cars in the USA - SVM & LR Classification\n",
    "#### By: David Wei, Sophia Wu, Dhruba Dey, Queena Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this section we will continue using our used car dataset and be building out a classification model using Logistic Regression (LR) and Support Vector Machines (SVM). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing libraries and reading in file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') #ignoring warnings\n",
    "import missingno as msno\n",
    "\n",
    "#plotting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotnine.data import economics\n",
    "from plotnine import ggplot, aes, geom_line\n",
    "\n",
    "#general sklearn libraries\n",
    "from scipy.stats import trim_mean, kurtosis\n",
    "from scipy.stats.mstats import mode, gmean, hmean\n",
    "import ptitprince as pt\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.model_selection as cross_validation\n",
    "import sklearn.linear_model as linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler #for scaling\n",
    "\n",
    "#logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics as mt\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience and clarity, we have exported all of the data tidying and cleaning we have applied to our original dataset from our initial EDA workbook as an importable file. This will allow us to simply pick up on where we left off without cluttering our notebook with all prior code. For reference, please refer to the following [github link](https://github.com/chee154/ml-Py-used_cars/blob/main/Used_Car_Lab_1_DataVisualization.ipynb) where all work has been contained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Records: 697989\n",
      "# of Columns: 18\n"
     ]
    }
   ],
   "source": [
    "df_raw = pd.read_csv(r'E:\\Data Files\\used_cars_data_cleaned.csv')\n",
    "print('# of Records: '+str(len(df_raw)))\n",
    "print('# of Columns: '+str(df_raw.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Tidying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Addressing Empty Values\n",
    "<b>NOTE</b>: First time running the logistic regression model returned an error: \n",
    "<br>\n",
    "*ValueError: Input contains NaN, infinity or a value too large for dtype('float64').*\n",
    "<br>\n",
    "Therefore, we will fix this by once again removing any strangling NA records from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "697881\n",
      "# of Records Removed: 108\n"
     ]
    }
   ],
   "source": [
    "df_cleaned = df_raw.copy()\n",
    "df_cleaned = df_cleaned.dropna()\n",
    "print(len(df_cleaned))\n",
    "print('# of Records Removed: '+str(len(df_raw)-len(df_cleaned)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning up final dataframe\n",
    "After the initial EDA, we will make the appropriate adjustments to our dataset based on our findings. To begin, let's get look at the relationship between **'length'** and **'height'** due to it's high correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "they are the same... dropping one\n",
      "17\n"
     ]
    }
   ],
   "source": [
    "df_test = df_cleaned.copy()\n",
    "print(df_test.shape[1])\n",
    "\n",
    "if df_test['length'].equals(df_test['height']) == True:\n",
    "    print('they are the same... dropping one')\n",
    "    df_test = df_test.drop(columns='height')\n",
    "    print(df_test.shape[1])\n",
    "else: \n",
    "    print('they are not the same')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformations for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['rank']=df_test['price'].rank()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['rank_pct']= df_test['price'].rank(pct=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[['price', 'rank', 'rank_pct']].sort_values(by=['rank'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_class_func(price):\n",
    "    if price >= .75:\n",
    "        return \"A\"\n",
    "    elif .5 <= price <= .74:\n",
    "        return \"B\"\n",
    "    elif .25 <= price <= .49:\n",
    "        return \"C\"\n",
    "    elif price <= .25:\n",
    "        return \"D\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['price_cat']=df_test['rank_pct'].apply(apply_class_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.groupby(['price_cat']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_test[['price', 'rank', 'rank_pct','price_cat']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transforming Response Variable from Continuous to Categorical\n",
    "Since our main interest in this dataset is the 'price' of a vehicle, we will transform our continuous price attribute into a categorical one by grouping all car prices into the following:\n",
    "* \"<5000\"          : price < 5000\n",
    "* \"5000-10000\"     : 5000 <= price <= 10000\n",
    "* \"10000-15000\"    : 10000 < price <= 15000\n",
    "* \"15000-20000\"    : 15000 < price <= 20000\n",
    "* \"20000-25000\"    : 20000 < price <= 25000\n",
    "* \"25000 and over\" : price > 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test2 = df_cleaned.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_group = []\n",
    "for price in df_test2[\"price\"]:\n",
    "    if price < 5000:\n",
    "        price_group.append(\"<5000\")\n",
    "    elif 5000 <= price <= 10000:\n",
    "        price_group.append(\"5000-10000\")\n",
    "    elif 10000 < price <= 15000:\n",
    "        price_group.append(\"10000-15000\")\n",
    "    elif 15000 < price <= 20000:\n",
    "        price_group.append(\"15000-20000\")\n",
    "    elif 20000 < price <= 25000:\n",
    "        price_group.append(\"20000-25000\")\n",
    "    else:\n",
    "        price_group.append(\"25000 and over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0            10000-15000\n",
      "1         25000 and over\n",
      "2            20000-25000\n",
      "3            20000-25000\n",
      "4         25000 and over\n",
      "               ...      \n",
      "697984       15000-20000\n",
      "697985       20000-25000\n",
      "697986       15000-20000\n",
      "697987    25000 and over\n",
      "697988       15000-20000\n",
      "Name: price_group, Length: 697881, dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_price_group = df_test2.copy()\n",
    "df_price_group[\"price_group\"] = price_group\n",
    "del df_price_group[\"price\"]\n",
    "print(df_price_group['price_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "price_group\n",
       "10000-15000        96001\n",
       "15000-20000       172368\n",
       "20000-25000       115254\n",
       "25000 and over    268595\n",
       "5000-10000         40482\n",
       "<5000               5181\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_price_group.groupby(['price_group']).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OneHotEncoding\n",
    "Once the data has been imported and cleaned, we will work on transforming our dataset to be more useful for our classification models. To start we will first one-hot encode all of our categorical (object) datatypes as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_encode_features(df):\n",
    "    result = df_cleaned.copy()\n",
    "    encoders = {}\n",
    "    for column in result.columns:\n",
    "        if result.dtypes[column] == np.object or result.dtypes[column]==np.bool:\n",
    "            encoders[column] = preprocessing.LabelEncoder()\n",
    "            result[column] = encoders[column].fit_transform(result[column])\n",
    "    print('Columns converted: '+str(encoders))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows a snap shot of what the final data looks like after categorical data has been encoded.\n",
    "You can see the body type is in a numerical representation, instead of a string (object) type, before being encoded.\n",
    "<br>\n",
    "- Below shows a snap shot of what the final data looks like after categorical data has been encoded.\n",
    "- You can see the body type is in a numerical representation, instead of a string (object) type, before being encoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns converted: {'body_type': LabelEncoder(), 'frame_damaged': LabelEncoder(), 'has_accidents': LabelEncoder(), 'is_new': LabelEncoder()}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>body_type</th>\n",
       "      <th>city_fuel_economy</th>\n",
       "      <th>daysonmarket</th>\n",
       "      <th>engine_displacement</th>\n",
       "      <th>frame_damaged</th>\n",
       "      <th>has_accidents</th>\n",
       "      <th>height</th>\n",
       "      <th>highway_fuel_economy</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>is_new</th>\n",
       "      <th>length</th>\n",
       "      <th>maximum_seating</th>\n",
       "      <th>mileage</th>\n",
       "      <th>owner_count</th>\n",
       "      <th>price</th>\n",
       "      <th>seller_rating</th>\n",
       "      <th>width</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>27.0</td>\n",
       "      <td>55</td>\n",
       "      <td>1500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.6</td>\n",
       "      <td>36.0</td>\n",
       "      <td>160.0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>42394.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>14639.0</td>\n",
       "      <td>3.447761</td>\n",
       "      <td>73.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>18.0</td>\n",
       "      <td>36</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>311.0</td>\n",
       "      <td>0</td>\n",
       "      <td>55.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>62251.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>32000.0</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>81.5</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>27</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.7</td>\n",
       "      <td>27.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.7</td>\n",
       "      <td>8.0</td>\n",
       "      <td>36410.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>23723.0</td>\n",
       "      <td>3.447761</td>\n",
       "      <td>78.6</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>15.0</td>\n",
       "      <td>27</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>69.9</td>\n",
       "      <td>22.0</td>\n",
       "      <td>281.0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.9</td>\n",
       "      <td>8.0</td>\n",
       "      <td>36055.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22422.0</td>\n",
       "      <td>3.447761</td>\n",
       "      <td>78.5</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>18.0</td>\n",
       "      <td>24</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>295.0</td>\n",
       "      <td>0</td>\n",
       "      <td>69.3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25745.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29424.0</td>\n",
       "      <td>3.447761</td>\n",
       "      <td>84.8</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697984</th>\n",
       "      <td>5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>32</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7444.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17836.0</td>\n",
       "      <td>4.533333</td>\n",
       "      <td>69.9</td>\n",
       "      <td>2019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697985</th>\n",
       "      <td>5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>17</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.4</td>\n",
       "      <td>32.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0</td>\n",
       "      <td>66.4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20160.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20700.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>80.0</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697986</th>\n",
       "      <td>6</td>\n",
       "      <td>26.0</td>\n",
       "      <td>17</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.9</td>\n",
       "      <td>37.0</td>\n",
       "      <td>179.0</td>\n",
       "      <td>0</td>\n",
       "      <td>57.9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>62138.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>17700.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>72.0</td>\n",
       "      <td>2018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697987</th>\n",
       "      <td>4</td>\n",
       "      <td>18.0</td>\n",
       "      <td>89</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>278.0</td>\n",
       "      <td>0</td>\n",
       "      <td>70.6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>20009.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>40993.0</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>75.2</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>697988</th>\n",
       "      <td>5</td>\n",
       "      <td>26.0</td>\n",
       "      <td>17</td>\n",
       "      <td>2500.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>68.1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>170.0</td>\n",
       "      <td>0</td>\n",
       "      <td>68.1</td>\n",
       "      <td>7.0</td>\n",
       "      <td>22600.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>19900.0</td>\n",
       "      <td>4.333333</td>\n",
       "      <td>72.4</td>\n",
       "      <td>2017</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>697881 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        body_type  city_fuel_economy  daysonmarket  engine_displacement  \\\n",
       "0               6               27.0            55               1500.0   \n",
       "1               1               18.0            36               3500.0   \n",
       "2               5               18.0            27               3600.0   \n",
       "3               5               15.0            27               3600.0   \n",
       "4               5               18.0            24               3600.0   \n",
       "...           ...                ...           ...                  ...   \n",
       "697984          5               26.0            32               1400.0   \n",
       "697985          5               26.0            17               2500.0   \n",
       "697986          6               26.0            17               2500.0   \n",
       "697987          4               18.0            89               3500.0   \n",
       "697988          5               26.0            17               2500.0   \n",
       "\n",
       "        frame_damaged  has_accidents  height  highway_fuel_economy  \\\n",
       "0                   0              0    57.6                  36.0   \n",
       "1                   0              0    55.1                  24.0   \n",
       "2                   0              0    70.7                  27.0   \n",
       "3                   0              1    69.9                  22.0   \n",
       "4                   0              0    69.3                  25.0   \n",
       "...               ...            ...     ...                   ...   \n",
       "697984              0              0    66.0                  31.0   \n",
       "697985              0              0    66.4                  32.0   \n",
       "697986              0              0    57.9                  37.0   \n",
       "697987              0              0    70.6                  23.0   \n",
       "697988              0              0    68.1                  33.0   \n",
       "\n",
       "        horsepower  is_new  length  maximum_seating  mileage  owner_count  \\\n",
       "0            160.0       0    57.6              5.0  42394.0          1.0   \n",
       "1            311.0       0    55.1              4.0  62251.0          1.0   \n",
       "2            310.0       0    70.7              8.0  36410.0          1.0   \n",
       "3            281.0       0    69.9              8.0  36055.0          1.0   \n",
       "4            295.0       0    69.3              5.0  25745.0          1.0   \n",
       "...            ...     ...     ...              ...      ...          ...   \n",
       "697984       138.0       0    66.0              5.0   7444.0          1.0   \n",
       "697985       170.0       0    66.4              5.0  20160.0          1.0   \n",
       "697986       179.0       0    57.9              5.0  62138.0          1.0   \n",
       "697987       278.0       0    70.6              5.0  20009.0          1.0   \n",
       "697988       170.0       0    68.1              7.0  22600.0          1.0   \n",
       "\n",
       "          price  seller_rating  width  year  \n",
       "0       14639.0       3.447761   73.0  2018  \n",
       "1       32000.0       2.800000   81.5  2018  \n",
       "2       23723.0       3.447761   78.6  2018  \n",
       "3       22422.0       3.447761   78.5  2017  \n",
       "4       29424.0       3.447761   84.8  2018  \n",
       "...         ...            ...    ...   ...  \n",
       "697984  17836.0       4.533333   69.9  2019  \n",
       "697985  20700.0       4.333333   80.0  2017  \n",
       "697986  17700.0       4.333333   72.0  2018  \n",
       "697987  40993.0       5.000000   75.2  2017  \n",
       "697988  19900.0       4.333333   72.4  2017  \n",
       "\n",
       "[697881 rows x 18 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data = number_encode_features(df_cleaned)\n",
    "encoded_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Referncing our transformed variable 'price_group' as an int to it's original string value for future analysis and interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.groupby(['price_group']).size())\n",
    "print('-----------------------------------------')\n",
    "print(df_price_group.groupby(['price_group']).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready for some model building!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of Records: 697881\n",
      "# of Columns: 18\n",
      "\n",
      "body_type                 int32\n",
      "city_fuel_economy       float64\n",
      "daysonmarket              int64\n",
      "engine_displacement     float64\n",
      "frame_damaged             int64\n",
      "has_accidents             int64\n",
      "height                  float64\n",
      "highway_fuel_economy    float64\n",
      "horsepower              float64\n",
      "is_new                    int64\n",
      "length                  float64\n",
      "maximum_seating         float64\n",
      "mileage                 float64\n",
      "owner_count             float64\n",
      "price                   float64\n",
      "seller_rating           float64\n",
      "width                   float64\n",
      "year                      int64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "df_final = encoded_data.copy()\n",
    "print('# of Records: '+str(len(df_final)))\n",
    "print('# of Columns: '+str(df_final.shape[1]))\n",
    "print()\n",
    "print(df_final.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Split\n",
    "Once our dataset ready for modeling, we will move on to our next steps of splitting up our data. For our dataset, we will use a 70:30 split that roughly leaves our training set with 488k records and test set with the remainder (209k records). We will then apply a 3-fold Cross Validation with a seed of 42 because it (42) is the answer to the ultimate question of life, the universe, and everything.\n",
    "<br><br>\n",
    "Our resposne variable will be price, more specifically the price group ('price_group') a car falls in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[[6.00000000e+00 2.70000000e+01 5.50000000e+01 ... 3.44776119e+00\n",
      "  7.30000000e+01 2.01800000e+03]\n",
      " [1.00000000e+00 1.80000000e+01 3.60000000e+01 ... 2.80000000e+00\n",
      "  8.15000000e+01 2.01800000e+03]\n",
      " [5.00000000e+00 1.80000000e+01 2.70000000e+01 ... 3.44776119e+00\n",
      "  7.86000000e+01 2.01800000e+03]\n",
      " ...\n",
      " [6.00000000e+00 2.60000000e+01 1.70000000e+01 ... 4.33333333e+00\n",
      "  7.20000000e+01 2.01800000e+03]\n",
      " [4.00000000e+00 1.80000000e+01 8.90000000e+01 ... 5.00000000e+00\n",
      "  7.52000000e+01 2.01700000e+03]\n",
      " [5.00000000e+00 2.60000000e+01 1.70000000e+01 ... 4.33333333e+00\n",
      "  7.24000000e+01 2.01700000e+03]]\n",
      "ShuffleSplit(n_splits=3, random_state=42, test_size=0.3, train_size=None)\n"
     ]
    }
   ],
   "source": [
    "if 'has_accidents' in df_final:\n",
    "    y = df_final['has_accidents'].values # get the labels we want\n",
    "    del df_final['has_accidents'] # get rid of the class label\n",
    "    X = df_final.values # use everything else to predict!\n",
    "print(y)\n",
    "print(X)\n",
    "\n",
    "num_cv_iterations = 3\n",
    "num_instances = len(y)\n",
    "cv_object = ShuffleSplit(n_splits=num_cv_iterations,random_state=42, test_size= 0.3)     \n",
    "print(cv_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf = LogisticRegression(penalty='l2', C=1.0, class_weight=None, solver='liblinear' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====Iteration 0  ====\n",
      "accuracy 0.8788479449764764\n",
      "confusion matrix\n",
      " [[183886    184]\n",
      " [ 25181    114]]\n",
      "====Iteration 1  ====\n",
      "accuracy 0.8775201203639577\n",
      "confusion matrix\n",
      " [[183600    206]\n",
      " [ 25437    122]]\n",
      "====Iteration 2  ====\n",
      "accuracy 0.8775153440164306\n",
      "confusion matrix\n",
      " [[183611    202]\n",
      " [ 25442    110]]\n"
     ]
    }
   ],
   "source": [
    "iter_num=0\n",
    "\n",
    "for train_indices, test_indices in cv_object.split(X,y): \n",
    "    # I will create new variables here so that it is more obvious what \n",
    "    # the code is doing (you can compact this syntax and avoid duplicating memory,\n",
    "    # but it makes this code less readable)\n",
    "    X_train = X[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    \n",
    "    X_test = X[test_indices]\n",
    "    y_test = y[test_indices]\n",
    "    \n",
    "    # train the reusable logisitc regression model on the training data\n",
    "    lr_clf.fit(X_train,y_train)  # train object\n",
    "    y_hat = lr_clf.predict(X_test) # get test set precitions\n",
    "\n",
    "    # now let's get the accuracy and confusion matrix for this iterations of training/testing\n",
    "    acc = mt.accuracy_score(y_test,y_hat)\n",
    "    conf = mt.confusion_matrix(y_test,y_hat)\n",
    "    print(\"====Iteration\",iter_num,\" ====\")\n",
    "    print(\"accuracy\", acc )\n",
    "    print(\"confusion matrix\\n\",conf)\n",
    "    iter_num+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of Training Set: 488516\n",
      "Count of Test Set: 209365\n"
     ]
    }
   ],
   "source": [
    "print('Count of Training Set: '+str(len(train_indices)))\n",
    "print('Count of Test Set: '+ str(len(test_indices)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we can change some of the parameters interactively\n",
    "from ipywidgets import widgets as wd\n",
    "\n",
    "def lr_explor(cost):\n",
    "    lr_clf = LogisticRegression(penalty='l2', C=cost, class_weight=None,solver='liblinear') # get object\n",
    "    accuracies = cross_val_score(lr_clf,X,y=y,cv=cv_object) # this also can help with parallelism\n",
    "    print(accuracies)\n",
    "\n",
    "wd.interact(lr_explor,cost=(0.001,5.0,0.05),__manual=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LR - Analysis & Intepretations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights *before* normalization:**\n",
    "<br>\n",
    "We can see that even before we normalized our dataset the **'mileage'** and **'frame_damaged'** attribute showns an extremely strong weight in regards to the price (and group) of a vehicle. Additionally, we can see that **'is_new'** has a particularly strong negative weight in regards to price, this can be interpreted that this variable less of an effect on pricing, which makes sense since prior analysis (lab 1) showed us that 99% of our data is used cars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "body_type has weight of -5.221811619310908e-07\n",
      "city_fuel_economy has weight of -3.0408958132218724e-05\n",
      "daysonmarket has weight of 0.0003073801827651254\n",
      "engine_displacement has weight of 4.282246898186513e-05\n",
      "frame_damaged has weight of 2.9894079244211207e-06\n",
      "height has weight of -0.00011599391734123565\n",
      "highway_fuel_economy has weight of -7.198702431440321e-06\n",
      "horsepower has weight of 0.000651767010988677\n",
      "is_new has weight of -1.350836635398382e-07\n",
      "length has weight of -0.00011599391734123565\n",
      "maximum_seating has weight of -9.229635995469008e-06\n",
      "mileage has weight of 7.938259602998602e-06\n",
      "owner_count has weight of 3.3831162980863724e-05\n",
      "price has weight of -3.2277191379042465e-05\n",
      "seller_rating has weight of -1.2858786938423712e-05\n",
      "width has weight of -8.766885179295116e-05\n",
      "year has weight of -0.0009708500382388764\n"
     ]
    }
   ],
   "source": [
    "weights = lr_clf.coef_.T # take transpose to make a column vector\n",
    "variable_names = df_final.columns\n",
    "for coef, name in zip(weights,variable_names):\n",
    "    print(name, 'has weight of', coef[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Weights *after* normalization:**\n",
    "<br>\n",
    "Once the weights have been normalized as shown below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.8776299763570797\n",
      "[[183418    395]\n",
      " [ 25225    327]]\n"
     ]
    }
   ],
   "source": [
    "# scale attributes by the training set\n",
    "scl_obj = StandardScaler()\n",
    "scl_obj.fit(X_train)\n",
    "\n",
    "X_train_scaled = scl_obj.transform(X_train) # apply to training\n",
    "X_test_scaled = scl_obj.transform(X_test) # apply those means and std to the test set (without snooping at the test set values)\n",
    "\n",
    "# train the model just as before\n",
    "lr_clf = LogisticRegression(penalty='l2', C=0.05, solver='liblinear') # get object, the 'C' value is less (can you guess why??)\n",
    "lr_clf.fit(X_train_scaled,y_train)  # train object\n",
    "\n",
    "y_hat = lr_clf.predict(X_test_scaled) # get test set precitions\n",
    "\n",
    "acc = mt.accuracy_score(y_test,y_hat)\n",
    "conf = mt.confusion_matrix(y_test,y_hat)\n",
    "print('accuracy:', acc )\n",
    "print(conf )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "price has weight of -0.402318886608593\n",
      "year has weight of -0.10059202119938179\n",
      "city_fuel_economy has weight of -0.08709642395116673\n",
      "seller_rating has weight of -0.062478778628759626\n",
      "width has weight of -0.05783510801209675\n",
      "engine_displacement has weight of -0.04383185665717114\n",
      "is_new has weight of -0.03939158028027939\n",
      "daysonmarket has weight of 0.009779825039265097\n",
      "body_type has weight of 0.014761435600783945\n",
      "height has weight of 0.015202113070669501\n",
      "length has weight of 0.015202113070669501\n",
      "maximum_seating has weight of 0.01706636631521288\n",
      "frame_damaged has weight of 0.08502806586538414\n",
      "owner_count has weight of 0.10268263727046947\n",
      "highway_fuel_economy has weight of 0.12472064979266269\n",
      "horsepower has weight of 0.15883455698304672\n",
      "mileage has weight of 0.20513927741025922\n"
     ]
    }
   ],
   "source": [
    "# sort these attributes and spit them out\n",
    "zip_vars = zip(lr_clf.coef_.T,df_final.columns) # combine attributes\n",
    "zip_vars = sorted(zip_vars) \n",
    "\n",
    "for coef, name in zip_vars:\n",
    "    print(name, 'has weight of', coef[0]) # now print them out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.01476144]\n",
      " [-0.08709642]\n",
      " [ 0.00977983]\n",
      " [-0.04383186]\n",
      " [ 0.08502807]\n",
      " [ 0.01520211]\n",
      " [ 0.12472065]\n",
      " [ 0.15883456]\n",
      " [-0.03939158]\n",
      " [ 0.01520211]\n",
      " [ 0.01706637]\n",
      " [ 0.20513928]\n",
      " [ 0.10268264]\n",
      " [-0.40231889]\n",
      " [-0.06247878]\n",
      " [-0.05783511]\n",
      " [-0.10059202]]\n"
     ]
    }
   ],
   "source": [
    "print(lr_clf.coef_.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

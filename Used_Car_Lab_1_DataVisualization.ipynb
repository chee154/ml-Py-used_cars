{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Used Cars in the USA\n",
    "#### By: David Wei, Sophia Wu, Dhruba Dey, Queena Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Business Understanding:\n",
    "As a team, we decided to work on a dataset which can easily be understood from our common life experience without needing specific domain knowledge such as finance, marketing and so forth. Secondly, we wanted to dabble ourselves with a dataset, that we had not previously exposed to, such as Real Estate, Life expectancy, etc. Off course, we must meet the project guidelines of 30,000 rows and 10 features, which in fact prompted us to look for somewhat large dataset with multiple features so that can effectively cull right set of features having numerical, boolean and categorical values. In addition, working with a large dataset would provide us with the experience that we have not acquired yet. This thought process of data selection has led us to grab “Used Car Dataset”, which has 3 million rows and 66 features. \n",
    "<br>\n",
    "<br>\n",
    "We wanted to build 3 different types of models on this dataset. First is Regression, second binary classification using Logistic Regression and third multiple classification using either K-nearest or K-means Random Forest depending on the best fit. We picked “Price” as the response feature for Regression model because it has numerical value and a pivotal point in the used car purchase. For Logistic Regression, we opted for “has_accidents” as the response variable because it has boolean values and an interesting point to observe whether we can effectively classify the used car with accidents. Finally, we used “body_type” as the target variable for multi-value classification. “Body_type” has 9 attributes which we believed a good candidate for multi-class classification. \n",
    "<br>\n",
    "<br>\n",
    "We applied first our domain knowledge, which is our collective experience of used car buying, to mine features that we thought would be relevant for making decision on a used car purchase. That helped us reduce the numbers of features from 66 to 40.  Then we took a close at each feature and tried to eliminate as many as possibleto keep the data size manageable so that we could handle data wrangling and model building, to be performed in the next phase, with the computing resources available to us. Also, we provided the justification as to why a feature was removed from the list of 40. However, we were cautious about categorical variable because itwould spawn to multiple variables after encoding. That is the key reason why we restricted the categorical feature to one. In the final dataset, we have 19 variables comprising 15 numerical, 1 boolean and 3 categorical. By the way, removing the missing values after sub-setting the data set to the intended features, we reduced the numbers of rows close to 700K from 3M. And we would also conduct PCA and Regularizations for dimensionality reduction. Based on the analysis and understanding of the data, we believed we selected the right set of data for the purpose of this project. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install missingno\n",
    "#!pip install plotnine\n",
    "#pip install ptitprince"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importing libraries and reading in file\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore') #ignoring warnings\n",
    "\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from plotnine.data import economics\n",
    "from plotnine import ggplot, aes, geom_line\n",
    "\n",
    "from scipy.stats import trim_mean, kurtosis\n",
    "from scipy.stats.mstats import mode, gmean, hmean\n",
    "import ptitprince as pt\n",
    "\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import sklearn.model_selection as cross_validation\n",
    "import sklearn.linear_model as linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# NOTE: need to change file per user\n",
    "df_raw = pd.read_csv(r'C:\\Data\\used_cars_data.csv')\n",
    "#df_raw = pd.read_csv('https://raw.githubusercontent.com/chee154/ml-Py-used_cars/main/data/kaggle_used_cars_data.csv')\n",
    "#df_raw = pd.read_csv('C:/Users/Queen/Documents/GitHub/ml-Py-used_cars/data/kaggle_used_cars_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_raw.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(\"Total # of Records: \" + str(df_raw.shape[0]))\n",
    "print(\"Total # of Columns: \" + str(df_raw.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Meaning Type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Domain based Attribute Reduction\n",
    "Before we do a deep dive into the types of our data, we will first look into reducing it from a  The total dataset has 66 attributes After a quick observation of the column headers, we can deduce that not all columns will be necessary for our analysis. Reasons for removing them below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#subsetting columns by referencing the column indexes\n",
    "df_cln_1 = df_raw.iloc[:, np.r_[0,5,7:11,13:17,18:21,22:30,32,35:38,42:50,51,55:57,61:63,64:66]]\n",
    "print(df_cln_1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "msno.matrix(df_cln_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After doing a quick profiling on some our identified columsn, we can see that both engine_cylinders and engine_type are the same. Additionally, we also found that the prefixes and suffixes attached to them are descriptive of it and thus not a continuous value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# removing city_fuel_economy since a quick vizualition of our dataset shows that ALL values are empty\n",
    "df_cln_1 = df_cln_1.drop(columns='combine_fuel_economy')\n",
    "print(df_cln_1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also found that 'engine_cylinders','engine_type' contains the same data in two separate columns, so we first test if this condition is true and if it is, we will remove one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#observing if 'engine_cylinders','engine_type' is the same data\n",
    "if df_cln_1['engine_cylinders'].equals(df_cln_1['engine_type']) == True:\n",
    "    df_cln_1 = df_cln_1.drop(columns='engine_cylinders')\n",
    "    print(df_cln_1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also found additional columns that were simply descriptions of another columns, for example. The 'wheel_system_display' attribute is simply a longer, more descriptive version of the 'wheel_system' attribute (\"Front-Wheel Drive\" vs \"FWD\"). For columns that follow this trend, we will remove the descriptive column from our dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(df_cln_1[['wheel_system']])\n",
    "print(df_cln_1[['wheel_system_display']])\n",
    "df_cln_1 = df_cln_1.drop(columns='wheel_system_display')\n",
    "print(df_cln_1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also discussed that 'interior_color' attribute has 45,726 distinct color values and so due to the sheer volume and complexity (# of levels in this attribute). We also decided to remove it from our dataset since it is unrealistic for the type of modeling we are doing and would effect the overall performance of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#note this plot takes forever to run\n",
    "#(ggplot(df_cln_1)+aes(x=\"interior_color\", y=\"price\")+geom_line())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_cln_1['interior_color'].nunique())\n",
    "\n",
    "df_cln_1 = df_cln_1.drop(columns='interior_color')\n",
    "print(df_cln_1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another attribute we found that could be removed as the \"listed_date\" attribute. Since there was no other 'datetype' attributes available in our dataset, we didn't really see a point with keeping it as there was no other data reference to use it with. Additionally, the attribute 'daysonmarket' already pre-aggregated the number of days it took to sell a vehicle, which the listed date would've been used for otherwise. Because of this, we will also remove it from our dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cln_1 = df_cln_1.drop(columns='listed_date')\n",
    "print(df_cln_1.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we decided to take a break from subsetting our data and take a look at the amount of missing values in the current dataframe. Our intention in mind was to see what the data looked like after it was cleaned to further proceed reducing the amount of attributes we had."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - Duplicates, Missing Data,  Nulls\n",
    "Now that our datatypes have been adjusted. We will work on cleaning up any empty data in our dataset. To begin, we will check if any of the VINs have duplicates in the dataset, since this the VIN is unique to a car we are expecting there shouldn't be, but in cases there is, we will remove it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_cln_1[df_cln_1.duplicated(['vin'], keep=False)].sort_values('vin')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that that there are duplicates even though the 'vin' should be specific and distinct to each car. There is a total of 80 records that contain duplicates. We will then remove these duplicates while keeping the 'first' record so that one copy of the duplcites will remain. We can see that after we cleaned these duplicates, our total # of records drop from 3,000,040 to 3,000,000. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(df_cln_1.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cln_1 = df_cln_1.drop_duplicates(subset=['vin'], keep='first')\n",
    "print(\"# of records after removing duplicates: \"+str(df_cln_1.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have cleaned our duplicates. A quick visualization of our data shows that almost every column has empty values. We will focus on analyzing those that have a large amount of empty data (ex. Frame_Damaged, has_accidents, isCab, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#visualizatin of our data BEFORE removing all rows with missing data\n",
    "msno.matrix(df_cln_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# remove the missing values\n",
    "df_cln_2 = df_cln_1.dropna()\n",
    "print(len(df_cln_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking the row counts of columns to see the missing rows\n",
    "df_cln_2.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#visualizatin of our data AFTER removing all rows with missing data\n",
    "msno.matrix(df_cln_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided that since our original dataset was large (10gb with 3million records), that instead of imputing data based on the mean or other statistical types, that deleting all records with empty values still returned 700,000 total records that not only provides plenty of data leftover, but also raw data that isn't imputed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning - Datatypes\n",
    "Obviously at this point we need to convert a few of our data columns to the appropriate data type by removing parts of the value string that we do not need such as \"gal\" in the ful_tank_volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finding all unique values per column to see what values we need to clean\n",
    "columns_that_need_cleaning = ['engine_type','exterior_color','frame_damaged','franchise_dealer','franchise_make','fuel_tank_volume','fuel_type','has_accidents','height','isCab','is_new','length','listing_color','make_name','maximum_seating','model_name','power','salvage','torque','transmission','wheel_system','width']\n",
    "for col in columns_that_need_cleaning:\n",
    "        print(df_cln_2[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finding all unique values per column to see what values we need to clean\n",
    "columns_that_need_cleaning_2 = ['fuel_tank_volume','height','length','maximum_seating','width',]\n",
    "for col in columns_that_need_cleaning_2:\n",
    "        print(df_cln_2[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regarding fuel_tank_volume and maximum_seating, we can see that there appears to be a pattern in the suffixes, \"gal\" and \"seats\" accordingly. We will now remove them and then convert all values to numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#removing unecesary string values in columns\n",
    "#then cleaning up any values that contain '--' and replacing it with NaN\n",
    "#Lastly, converting the value first to a string type and then to a float type\n",
    "\n",
    "df_cln_2['fuel_tank_volume']=df_cln_2['fuel_tank_volume'].astype(str).str.replace(' gal', '').replace('--',np.NaN).astype(float)\n",
    "df_cln_2['height']=df_cln_2['height'].astype(str).str.replace(' in', '').replace('--',np.NaN).astype(float)\n",
    "df_cln_2['length']=df_cln_2['height'].astype(str).str.replace(' in', '').replace('--',np.NaN).astype(float)\n",
    "df_cln_2['maximum_seating']=df_cln_2['maximum_seating'].astype(str).str.replace(' seats', '').replace('--',np.NaN).astype(float)\n",
    "df_cln_2['width']=df_cln_2['width'].astype(str).str.replace(' in', '').replace('--',np.NaN).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#doing a quick profile on the subsetted columns\n",
    "print(df_cln_2.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now see that all of our column values have been adjusted to the correct datatypes. We will next proceed with cleaning up the remainder of our data attributes. A quick count on our datatypes shows us that we still have 19 categorical values, 14 numerical types and 2 booleans (true/false) left to work wikth. Since our ultimate goal is doing a regression and one classification model, we will next work on trimming down our 'object' categorical data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#count of datatypes in current dataframe.\n",
    "print(df_cln_2.dtypes.value_counts())\n",
    "print(\"---------------------------------------------\")\n",
    "#showing only the object type columns\n",
    "print(df_cln_2.select_dtypes(include='object').columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the team decided to choose 'body_type' as main the main classification term. We decided to create a final dataframe with only the attributes we found useful for our model which excludes:\n",
    "- **'vin'**: acts as primary index but not useful for modeling\n",
    "<br>\n",
    "- **'city'**: no model of ours will require geographic features, also since there is not state or zip attributes, it is ambiguous\n",
    "<br>\n",
    "- **'engine_type'**: use for EDA but 'horsepower' is the continous version of this attribute\n",
    "<br>\n",
    "- **'franchise_dealer'**: not useful for our model \n",
    "<br>\n",
    "- **'franchise_make'**: same as 'make_name' which has too many levels as a categorical value and not necessary for our predictin model\n",
    "<br>\n",
    "- **'fuel_tank_volume'**: \n",
    "<br>\n",
    "- **'fuel_type'**: \n",
    "<br>\n",
    "- **'isCab'**:\n",
    "<br>\n",
    "- **'listing_color'**: \n",
    "<br>\n",
    "- **'make_name'**: \n",
    "<br>\n",
    "- **'model_name'**: \n",
    "<br>\n",
    "- **'power'**: 'horsepower' attribute will be used instead\n",
    "<br>\n",
    "- **'salvage'**: the value that is associated with depreciation number which is more of accounting. In real world, we pay what is the market value of the car.\n",
    "<br>\n",
    "- **'torque'**:  'horsepower' attribute will be used instead\n",
    "<br>\n",
    "- **'transmission'**: \n",
    "<br>\n",
    "- **'wheel_system'**: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_cln_2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#creating final df for analysis\n",
    "df_final = df_cln_2.iloc[:, np.r_[1,3:6,8,13,14:17,18:20,22:24,25,27,29,33:35]]\n",
    "print(df_final.info())\n",
    "print(df_final.dtypes.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Look at the final data, we have total 18 columns, their data types are: bool(1), float64(12), int64(2), object(3).\n",
    "\n",
    "As our main goal is to predict car price and car type,in the simple statistics we need to find out the 5 most significant attributes that affect the car price, which's divided to below 5 steps:\n",
    "<br>\n",
    "<br>\n",
    "1.Check the data range, mode, mean, median, variance and counts, etc\n",
    "<br>\n",
    "2.Check data normality\n",
    "<br>\n",
    "3.Correlation check\n",
    "<br>\n",
    "4.With the result from correlation checking,the 5 most significan attributes are horsepower(0.629),mileage(-0.43),highway_fuel_economy(-0.401), year(0.371) and engine_displacement(0.427)\n",
    "<br>\n",
    "<br>\n",
    "An intresting thing we found is that the seller_rating and owner_count don't show strong relationship to car price from correlation checking, they're only -0.219 and -0.035\n",
    "![title](img/correlationPic.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Median, Standard Deviation, Mean in a Pandas Dataframe\n",
    "We can see from observing the simple statistic is that the median price aligns with the IQR as most of the cars are priced at around $21,700. However, what's interesting to note here is that the though the median and the mean are somewhat similar, there is a standard deviation of about \\\\$15,878. This could be due to the fact that the extremities in car pricing varies greatly from the average with the lost car priced at \\\\$484 while the highest price car is at \\\\$3,299,995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final['price'].aggregate([np.median, np.std, np.mean]).reset_index())\n",
    "print('min: '+str(df_final['price'].min()))\n",
    "print('min: '+str(df_final['price'].max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inter-quartile Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_final.describe()['price'][['25%', '50%', '75%']])\n",
    "print(df_final.max()['price'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see that our dataset has the majority (~99%) of our cars being used cars as described in the title, with less than 1% being new."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the count of new cars\n",
    "print('count of new cars: '+str(df_final['is_new'].values.sum()))\n",
    "print('count of used cars: '+str((~df_final['is_new']).values.sum()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then looked into the number of different types'cars that have accidents or no accidents. We found that 84706 cars have accidents and out of that, Suv/Crossover and Sedan cars have higher numbers of accidents, as their count are larger too. Perhaps we could say love SUV and Sedan cars more than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the count of cars have accidents\n",
    "print(df_final['has_accidents'].values.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visulize the number of different type of cars that have accidents or not.\n",
    "sns.catplot(x=\"has_accidents\", kind=\"count\",hue='body_type', data=df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize the number of frame damaged cars\n",
    "sns.catplot(x=\"frame_damaged\", kind=\"count\", data=df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the count of frame_damaged cars\n",
    "df_final['frame_damaged'].values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the number of different type of cars,it looks like SUV is the most popular car with almost 3500, Sedan is the 2nd most popular car.\n",
    "sns.catplot(x=\"body_type\", kind=\"count\",data=df_final,height=8, aspect=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data normality check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##select all numberical data type from the final data\n",
    "df_final_num=df_final.iloc[:, np.r_[1:4,6:9,10:18]]\n",
    "print(df_final_num.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check all numerical type data's normality with pairplot\n",
    "sns.pairplot(df_final_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From the histogram, we may easily see  city_fuel_economy,engine_displacement and highway_fuel_economy more normally distributed than other columns. As our sample size is large enough,we use Central Limit Theorem rule and assume they're approximately normal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##data correlation with Spearman method\n",
    "df_final.corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data correlation check\n",
    "cormat = df_final.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triang(cormat, triang='lower'):\n",
    "    \n",
    "    if triang == 'upper':\n",
    "        rstri = pd.DataFrame(np.triu(cormat.values),\n",
    "                             index=cormat.index,\n",
    "                             columns=cormat.columns).round(3)\n",
    "        rstri = rstri.iloc[:,1:]\n",
    "        rstri.drop(rstri.tail(1).index, inplace=True)\n",
    "        \n",
    "    if triang == 'lower':\n",
    "        rstri = pd.DataFrame(np.tril(cormat.values),\n",
    "                             index=cormat.index,\n",
    "                             columns=cormat.columns).round(3)\n",
    "        rstri = rstri.iloc[:,:-1]\n",
    "        rstri.drop(rstri.head(1).index, inplace=True) \n",
    " \n",
    "    rstri.replace(to_replace=[0,1], value='', inplace=True)\n",
    "    \n",
    "    return(rstri)\n",
    "\n",
    "triang(cormat, triang='upper')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Result: We may see most significan attributes are Horsepower ,Mileage,highway_fuel_economy,Year,engine_displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Creating the Heatmap from the Post\n",
    "mask = np.zeros_like(cormat, dtype=np.bool)\n",
    "mask[np.triu_indices_from(mask)] = True\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(11, 10))\n",
    "sns.heatmap(cormat, mask=mask, vmax=.9, center=0,\n",
    "            square=True, linewidths=.6, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with Raincloud Plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "ax = pt.RainCloud(x = 'has_accidents', y = 'price', \n",
    "                  data = df_final, \n",
    "                  width_viol =  .5,\n",
    "                  width_box =  .9,\n",
    "                  orient = 'h',\n",
    "                  move = .0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pt.RainCloud(x = 'is_new', y = 'price', \n",
    "                  data = df_final, \n",
    "                  width_viol = .9,\n",
    "                  width_box = 3,\n",
    "                  orient = 'h',\n",
    "                  move = .0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This shows the new car is more expensive than used car, the car had accident is less expensive than the car without accident"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization with Pairplot using Seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['price', 'city_fuel_economy','engine_displacement','highway_fuel_economy','horsepower','mileage']\n",
    "ax = sns.pairplot(df_final, vars=cols, hue='has_accidents')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## From above result, we rerun it again without city fuel economy and highway_fuel economy\n",
    "cols = ['price', 'engine_displacement','horsepower','mileage','maximum_seating','owner_count','seller_rating','year']\n",
    "ax = sns.pairplot(df_final, vars=cols, hue='has_accidents')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Summary: The most significant attributes to the price are horsepower, mileage, year, highway_fuel_economy, engine_displacement, possible attributes are owner_count and seller_rating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the horsepower and price with Scatter plot using Seaborn lmplot.\n",
    "sns.lmplot(x='horsepower', y='price', hue='has_accidents', data=df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### It shows positive relationship between horsepower and price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the engine_displacement and price with Scatter plot using Seaborn lmplot.\n",
    "sns.lmplot(x='engine_displacement', y='price', hue='has_accidents', data=df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This shows positve relationship as well between price and engine_displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the higway_fuel_economy and price with Scatter plot using Seaborn lmplot.\n",
    "sns.lmplot(x='highway_fuel_economy', y='price', hue='has_accidents', data=df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This shows negtive relationship as well between price and highway_fuel_economy\n",
    "The lower the mileage, the higher the price, and that goes with the general notion. However, the relation breaks when the mileage goes beyond 50k as the price does not vary much from that point. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the relationship between mileage and price with Scatter plot using Seaborn lmplot.\n",
    "sns.lmplot(x='mileage', y='price', hue='has_accidents',palette=\"Set1\", markers='+', data=df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### This shows negtive relationship as well between price and mileage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Joint Attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When it comes to joint attributes, we would like to see if there's correlation between \"car body type\" and \"city_fuel_economy\".\n",
    "\n",
    "First, we asked: \n",
    "\n",
    "*What's the most popular **city fuel economy** for used SUV, pickup Truck, sedan and convertible?* (Please see Diagram 1)\n",
    "* The most popular \"city fuel economy\" for **used Car** overall is 18 mile\n",
    "* The most popular \"city fuel economy\" for **used SUV** is 22 mile\n",
    "* The most popular \"city fuel economy\" for **used Pickup Truck** is 15 mile\n",
    "* The most popular \"city fuel economy\" for **used Sedan** is 28 mile\n",
    "* The most popular \"city fuel economy\" for **used convertible** is 20 mile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_graph1 = df_final.groupby(['city_fuel_economy', 'body_type'])['body_type'].count().unstack().fillna(0)\n",
    "df_graph1.plot(figsize=(10,10), kind=\"bar\", stacked = True, ylabel = \"total number of used car sold\", title = \"Diagram 1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another question we asked was:\n",
    "\n",
    "*What's the **highest amount of seating** people are looking for when looking to buy a used car?* (Please see Diagram 2)\n",
    "\n",
    "* The **most popular** number of seats for used cars is 5 seats\n",
    "* The **second most popular** number of seats for used car is 6 seats\n",
    "* When buying a Sedan, customers mostly are looking for 5 seats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_graph2 = df_final.groupby(['maximum_seating', 'body_type'])['body_type'].count().unstack().fillna(0)\n",
    "df_graph2.plot(figsize=(10,10), kind=\"bar\", stacked = True, ylabel = \"total number of used car sold\", title = \"Diagram 2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore Attributes and Class\n",
    "we are using price (for regression), has_accidents(for logistic regression) and body_type(for multiclass classifications) as the response variables. In this section, we will try to find some interesting relations involving these features. \n",
    "\n",
    "'has-accidents' and 'is_new' are two booleans variables in our final dataset. We could have either or these these two values for the binary classification. But has_accidents feature is more balanced (false: 87% and true:13%) than that of is_new. That is the another driver, besides what is stated in the Business Understanding section, why we leaned on has_accidents for the binary response variable. However, we could have used is_new for the response variable. In that case, we should downscale or upscale, though the downscaling is preferred, training data, which would be an additional step. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--attribute counts of is_new--')\n",
    "print(df_final['is_new'].value_counts())\n",
    "\n",
    "print('--attribute counts of has_accidents--')\n",
    "print(df_final['has_accidents'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One may think the drivers of Sendan, Minivan or wagon may be more cautious that those who drive SUV or Pickup Truck. But at least based on used car market as presented in this visual, there is not much difference in the driving behavior amoung drives of different body types. However, the drivers of coupe and convertible exhibit restriant while on the road because the damage of their vehicle would cost them dearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Price_bodytype_accidents = sns.barplot(x=\"body_type\", y=\"price\", hue = \"has_accidents\",data=df_final)\n",
    "Price_bodytype_accidents.set_xticklabels(Price_bodytype_accidents.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coupe, SUV/Crossover, Convertible and Sedan have many outliers, whereas Wagon, Minivan and Vad do not have any outlier. In median used car price of 9 body types are with a close band. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Price_bodytype = sns.boxplot(x=\"body_type\", y=\"price\", data=df_final)\n",
    "Price_bodytype.set_xticklabels(Price_bodytype.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sedan, SUV/Crossover, Pickup Truck stay in the market much loger than Coupe, Wagon and Hackback do. It is surprise to see that Coupe, though it is an expensive category car, sells faster Sedan, which we believe a common people's car. Most likely supply demand or mispricing is causing this anamoly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot = sns.catplot(x=\"body_type\", y=\"daysonmarket\", data=df_final)\n",
    "for axes in plot.axes.flat:\n",
    "    _ = axes.set_xticklabels(axes.get_xticklabels(), rotation=90)\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may think that the used cars having accidents will stay longer in the market, but that is not the case with an exception of Van. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Plt3 = sns.barplot(x=\"body_type\", y=\"daysonmarket\", hue = \"has_accidents\", palette=\"deep\",data=df_final)\n",
    "Plt3.set_xticklabels(Plt3.get_xticklabels(), rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We further group numerical predictors, including \"price\", to help us to gain more insight. \n",
    "\n",
    "This new feature is created using the attribute 'price'. \n",
    "We divide price into different price groups as following:\n",
    "\n",
    "* \"<5000\"          : price < 5000\n",
    "* \"5000-10000\"     : 5000 <= price <= 10000\n",
    "* \"10000-15000\"    : 10000 < price <= 15000\n",
    "* \"15000-20000\"    : 15000 < price <= 20000\n",
    "* \"20000-25000\"    : 20000 < price <= 25000\n",
    "* \"25000 and over\" : price > 25000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "price_group = []\n",
    "for price in df_final[\"price\"]:\n",
    "    if price < 5000:\n",
    "        price_group.append(\"<5000\")\n",
    "    elif 5000 <= price <= 10000:\n",
    "        price_group.append(\"5000-10000\")\n",
    "    elif 10000 < price <= 15000:\n",
    "        price_group.append(\"10000-15000\")\n",
    "    elif 15000 < price <= 20000:\n",
    "        price_group.append(\"15000-20000\")\n",
    "    elif 20000 < price <= 25000:\n",
    "        price_group.append(\"20000-25000\")\n",
    "    else:\n",
    "        price_group.append(\"25000 and over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_df_price = df_final.copy()\n",
    "new_df_price[\"price_group\"] = price_group\n",
    "del new_df_price[\"price\"]\n",
    "sns.catplot(x=\"price_group\", y= \"body_type\", kind=\"bar\", data=new_df_price)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "Obervations after we had used the new price group for data analysing:\n",
    "* Over 65% of customers spent 25000 and more when buying a used Pickup truck\n",
    "* Over 50% of customers spent over 20000 for a used SUV\n",
    "* Over 50% of customers spent between 10000-20000 when buying a used sedan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_price = pd.crosstab(new_df_price.body_type, new_df_price.price_group)\n",
    "df_price.plot(figsize=(10,10), kind=\"bar\", stacked = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exceptional Work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will use *one hot encoding* to encode the data set below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn.preprocessing as preprocessing\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.model_selection as cross_validation\n",
    "import sklearn.linear_model as linear_model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we encode the categorical features as numbers. (see below function) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def number_encode_features(df_final):\n",
    "    result = df_final.copy()\n",
    "    encoders = {}\n",
    "    for column in result.columns:\n",
    "        if result.dtypes[column] == np.object:\n",
    "            encoders[column] = preprocessing.LabelEncoder()\n",
    "            result[column] = encoders[column].fit_transform(result[column])\n",
    "    return result, encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows a snap shot of what the final data looks like after categorical data has been encoded.\n",
    "You can see the body type is in a numerical representation, instead of a string (object) type, before being encoded.\n",
    "\n",
    "As indicated in the heatmap in the sample statistics (above) section, in the future we want to further explore the strong correlation between \"price\" and \"body_type\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "encoded_data = number_encode_features(df_final)\n",
    "encoded_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
